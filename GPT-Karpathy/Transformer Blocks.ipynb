{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a69e8ff",
   "metadata": {},
   "source": [
    "# GPT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af587370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "input_file_path = os.path.join('Data', 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd133d9",
   "metadata": {},
   "source": [
    "##### Tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63831f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just for character-level tokentization as i only have a mac M4\n",
    "# Sentencepiece is whats used commonly within the NLP community\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars) # gpt2 is around 50000\n",
    "\n",
    "# Encode and decode \n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d35bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([4, 16])\n",
      "Target batch shape: torch.Size([4, 16])\n",
      "Sample input: tensor([57, 58, 47, 50, 47, 58, 63,  0, 32, 53,  1, 57, 43, 43, 49,  1])\n",
      "Sample target: tensor([58, 47, 50, 47, 58, 63,  0, 32, 53,  1, 57, 43, 43, 49,  1, 58])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/792373076.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/792373076.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(chunk[1:], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a chunk of text of size block_size + 1\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "    \n",
    "        # Split into input and target\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "tokenized_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = len(tokenized_data)\n",
    "train_data = tokenized_data[:int(n*0.9)]\n",
    "val_data = tokenized_data[int(n*0.9):]\n",
    "\n",
    "train_dataset = CharDataset(train_data, block_size=16)\n",
    "val_dataset = CharDataset(val_data, block_size=16)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "    if batch_idx == 0:  # Just show the first batch\n",
    "        print(\"Input batch shape:\", x_batch.shape)\n",
    "        print(\"Target batch shape:\", y_batch.shape)\n",
    "        print(\"Sample input:\", x_batch[0])\n",
    "        print(\"Sample target:\", y_batch[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf77e9df",
   "metadata": {},
   "source": [
    "## BIgram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 65]) tensor(4.7988, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHj\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramlanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # used to pluck out the embedding of each input idx\n",
    "        # The second dimension is also vocab_size because each token's embedding is used directly as logits\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) (batch, time, channel) (4, 8, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets) # cross entripy exepects (B,C, T)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # Focus on the last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "blm= BigramlanguageModel(vocab_size)\n",
    "logits, loss = blm(x_batch, y_batch)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx = blm.generate(idx, max_new_tokens=50)[0].tolist()\n",
    "print(decode(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b270d22",
   "metadata": {},
   "source": [
    "This is a simplified approach where the model learns to predict the next token based solely on the current token's embedding, without any additional processing. In more complex models (like full Transformers), these embeddings would be further processed through multiple layers of self-attention and feed-forward networks before producing the final logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7283a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/792373076.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/792373076.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(chunk[1:], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 2.5626, Val Loss: 2.4875\n",
      "Epoch 2/5, Train Loss: 2.4526, Val Loss: 2.4894\n",
      "Epoch 3/5, Train Loss: 2.4525, Val Loss: 2.4905\n",
      "Epoch 4/5, Train Loss: 2.4525, Val Loss: 2.4898\n",
      "Epoch 5/5, Train Loss: 2.4525, Val Loss: 2.4901\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = torch.optim.AdamW(blm.parameters(), lr=1e-3)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    blm.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch, y_batch\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        _, loss = blm(x_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    blm.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch, y_batch\n",
    "            _, loss = blm(x_batch, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, ' \n",
    "          f'Train Loss: {avg_train_loss:.4f}, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49fe41c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BRDY LELERILA:\n",
      "Whistr.\n",
      "\n",
      "Age us r f istend aly n;\n",
      "\n",
      "ORole tars otat CATousur,\n",
      "\n",
      "Al lillong m ato s-bly,\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx = blm.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657b01d",
   "metadata": {},
   "source": [
    "##### The loss plateaus at 2.45. The model predictions do look like words but it still sucks because we are only predicting based on a context of one single token.\n",
    "-> > the objective is to broaden this context and see how that enhances our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a69257a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 65]) tensor(42.6758, grad_fn=<NllLossBackward0>)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "class BigramMeanModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "        \n",
    "        # Ensure we don't process more than block_size tokens\n",
    "        idx = idx[:, -self.block_size:]  # (B, min(T, block_size))\n",
    "        T = idx.shape[1]  # Update T to actual sequence length\n",
    "        \n",
    "        # Get token embeddings: (B, T, C)\n",
    "        token_embeddings = self.token_embeddings(idx)\n",
    "        \n",
    "        # Create a lower triangular matrix for masking (including current token)\n",
    "        mask = torch.tril(torch.ones(T, T, device=device))  # (T, T)\n",
    "        mask = mask.unsqueeze(0)  # (1, T, T)\n",
    "        \n",
    "        # Compute cumulative sum of embeddings and counts\n",
    "        cum_embeddings = torch.einsum('btc,bts->bsc', token_embeddings, mask)  # (B, T, C)\n",
    "        counts = mask.sum(dim=2, keepdim=True)  # (1, T, 1)\n",
    "        mean_embeddings = cum_embeddings / (counts + 1e-8)  # (B, T, C)\n",
    "        \n",
    "        # Project to vocabulary space\n",
    "        logits = mean_embeddings @ self.token_embeddings.weight.T  # (B, T, V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # For loss, we predict the next token\n",
    "            logits = logits[:, :-1, :]  # (B, T-1, V)\n",
    "            targets = targets[..., 1:T]  # (B, T-1)\n",
    "            \n",
    "            # Reshape for cross_entropy\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (B*(T-1), V)\n",
    "            targets_flat = targets.reshape(-1)  # (B*(T-1),)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # Get the predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Focus on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, V)\n",
    "            # Get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "block_size = 5  # context window\n",
    "bmm = BigramMeanModel(vocab_size, block_size)\n",
    "\n",
    "# This should now work with any sequence length\n",
    "x_batch = torch.randint(0, vocab_size, (4, 10))  # Batch of 4 sequences, each of length 10\n",
    "y_batch = torch.randint(0, vocab_size, (4, 10))\n",
    "\n",
    "logits, loss = bmm(x_batch, y_batch)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "# Initialize with a non-zero starting token\n",
    "context = torch.ones((8, 8), dtype=torch.long)  # Start with token 1\n",
    "generated = bmm.generate(context, max_new_tokens=10)\n",
    "print(generated[0].tolist())  # Convert to list for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0857f65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((8, 8), dtype=torch.long) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669c797",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819465cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11,  0,  6, 37, 14, 58, 37, 15],\n",
      "        [35, 54, 13, 23, 35,  3,  1, 42],\n",
      "        [49, 50,  6, 56,  2, 13, 31, 41],\n",
      "        [22, 19, 28, 23,  8, 18, 23, 19]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This should now work with any sequence length\n",
    "x_batch = torch.randint(0, vocab_size, (4, 8))  # Batch of 4 sequences, each of length 10\n",
    "y_batch = torch.randint(0, vocab_size, (4, 8))\n",
    "print(x_batch)  # Should print: torch.Size([4, 7, vocab_size]) tensor(...)\n",
    "\n",
    "logits, loss = bmm(x_batch, y_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f3848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
