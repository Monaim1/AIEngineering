{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a69e8ff",
   "metadata": {},
   "source": [
    "# GPT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af587370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "input_file_path = os.path.join('Data', 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd133d9",
   "metadata": {},
   "source": [
    "##### Tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63831f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just for character-level tokentization as i only have a mac M4\n",
    "# Sentencepiece is whats used commonly within the NLP community\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars) # gpt2 is around 50000\n",
    "\n",
    "# Encode and decode \n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d35bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([4, 16])\n",
      "Target batch shape: torch.Size([4, 16])\n",
      "Sample input: tensor([57, 58, 47, 50, 47, 58, 63,  0, 32, 53,  1, 57, 43, 43, 49,  1])\n",
      "Sample target: tensor([58, 47, 50, 47, 58, 63,  0, 32, 53,  1, 57, 43, 43, 49,  1, 58])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/792373076.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/792373076.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(chunk[1:], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a chunk of text of size block_size + 1\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "    \n",
    "        # Split into input and target\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "tokenized_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = len(tokenized_data)\n",
    "train_data = tokenized_data[:int(n*0.9)]\n",
    "val_data = tokenized_data[int(n*0.9):]\n",
    "\n",
    "train_dataset = CharDataset(train_data, block_size=16)\n",
    "val_dataset = CharDataset(val_data, block_size=16)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "    if batch_idx == 0:  # Just show the first batch\n",
    "        print(\"Input batch shape:\", x_batch.shape)\n",
    "        print(\"Target batch shape:\", y_batch.shape)\n",
    "        print(\"Sample input:\", x_batch[0])\n",
    "        print(\"Sample target:\", y_batch[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf77e9df",
   "metadata": {},
   "source": [
    "## BIgram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 65]) tensor(4.7988, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHj\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramlanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # used to pluck out the embedding of each input idx\n",
    "        # The second dimension is also vocab_size because each token's embedding is used directly as logits\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) (batch, time, channel) (4, 8, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets) # cross entripy exepects (B,C, T)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # Focus on the last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "blm= BigramlanguageModel(vocab_size)\n",
    "logits, loss = blm(x_batch, y_batch)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx = blm.generate(idx, max_new_tokens=50)[0].tolist()\n",
    "print(decode(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b270d22",
   "metadata": {},
   "source": [
    "This is a simplified approach where the model learns to predict the next token based solely on the current token's embedding, without any additional processing. In more complex models (like full Transformers), these embeddings would be further processed through multiple layers of self-attention and feed-forward networks before producing the final logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7283a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/792373076.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/792373076.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(chunk[1:], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 2.5626, Val Loss: 2.4875\n",
      "Epoch 2/5, Train Loss: 2.4526, Val Loss: 2.4894\n",
      "Epoch 3/5, Train Loss: 2.4525, Val Loss: 2.4905\n",
      "Epoch 4/5, Train Loss: 2.4525, Val Loss: 2.4898\n",
      "Epoch 5/5, Train Loss: 2.4525, Val Loss: 2.4901\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = torch.optim.AdamW(blm.parameters(), lr=1e-3)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    blm.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch, y_batch\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        _, loss = blm(x_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    blm.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch, y_batch\n",
    "            _, loss = blm(x_batch, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, ' \n",
    "          f'Train Loss: {avg_train_loss:.4f}, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49fe41c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BRDY LELERILA:\n",
      "Whistr.\n",
      "\n",
      "Age us r f istend aly n;\n",
      "\n",
      "ORole tars otat CATousur,\n",
      "\n",
      "Al lillong m ato s-bly,\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx = blm.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657b01d",
   "metadata": {},
   "source": [
    "##### The loss plateaus at 2.45. The model predictions do look like words but it still sucks because we are only predicting based on a context of one single token.\n",
    "-> > the objective is to broaden this context and see how that enhances our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a69257a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 65]) tensor(42.6758, grad_fn=<NllLossBackward0>)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "class BigramMeanModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "        \n",
    "        # Ensure we don't process more than block_size tokens\n",
    "        idx = idx[:, -self.block_size:]  # (B, min(T, block_size))\n",
    "        T = idx.shape[1]  # Update T to actual sequence length\n",
    "        \n",
    "        # Get token embeddings: (B, T, C)\n",
    "        token_embeddings = self.token_embeddings(idx)\n",
    "        \n",
    "        # Create a lower triangular matrix for masking (including current token)\n",
    "        mask = torch.tril(torch.ones(T, T, device=device))  # (T, T)\n",
    "        mask = mask.unsqueeze(0)  # (1, T, T)\n",
    "        \n",
    "        # Compute cumulative sum of embeddings and counts\n",
    "        cum_embeddings = torch.einsum('btc,bts->bsc', token_embeddings, mask)  # (B, T, C)\n",
    "        counts = mask.sum(dim=2, keepdim=True)  # (1, T, 1)\n",
    "        mean_embeddings = cum_embeddings / (counts + 1e-8)  # (B, T, C)\n",
    "        \n",
    "        # Project to vocabulary space\n",
    "        logits = mean_embeddings @ self.token_embeddings.weight.T  # (B, T, V)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # For loss, we predict the next token\n",
    "            logits = logits[:, :-1, :]  # (B, T-1, V)\n",
    "            targets = targets[..., 1:T]  # (B, T-1)\n",
    "            \n",
    "            # Reshape for cross_entropy\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (B*(T-1), V)\n",
    "            targets_flat = targets.reshape(-1)  # (B*(T-1),)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # Get the predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Focus on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, V)\n",
    "            # Get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "block_size = 5  # context window\n",
    "bmm = BigramMeanModel(vocab_size, block_size)\n",
    "\n",
    "# This should now work with any sequence length\n",
    "x_batch = torch.randint(0, vocab_size, (4, 10))  # Batch of 4 sequences, each of length 10\n",
    "y_batch = torch.randint(0, vocab_size, (4, 10))\n",
    "\n",
    "logits, loss = bmm(x_batch, y_batch)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "# Initialize with a non-zero starting token\n",
    "context = torch.ones((8, 8), dtype=torch.long)  # Start with token 1\n",
    "generated = bmm.generate(context, max_new_tokens=10)\n",
    "print(generated[0].tolist())  # Convert to list for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0857f65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((8, 8), dtype=torch.long) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669c797",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36a158",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec41905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0036, -0.0286],\n",
       "        [-1.1775, -0.6781],\n",
       "        [ 1.7286, -0.9082],\n",
       "        [-0.4907,  0.3508],\n",
       "        [ 1.3797, -0.8736],\n",
       "        [ 0.8935,  0.6892],\n",
       "        [ 1.2927, -0.1709],\n",
       "        [ 0.6404, -1.3544]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T, C= 4, 8, 2\n",
    "x=torch.randn(B,T, C)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819465cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0036, -0.0286],\n",
       "        [-0.5905, -0.3533],\n",
       "        [ 0.1825, -0.5383],\n",
       "        [ 0.0142, -0.3160],\n",
       "        [ 0.2873, -0.4275],\n",
       "        [ 0.3884, -0.2414],\n",
       "        [ 0.5175, -0.2313],\n",
       "        [ 0.5329, -0.3717]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wei = tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "793f3848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e27e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
