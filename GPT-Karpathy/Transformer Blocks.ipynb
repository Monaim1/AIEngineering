{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a69e8ff",
   "metadata": {},
   "source": [
    "# GPT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af587370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "input_file_path = os.path.join('Data', 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd133d9",
   "metadata": {},
   "source": [
    "##### Tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63831f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just for character-level tokentization as i only have a mac M4\n",
    "# Sentencepiece is whats used commonly within the NLP community\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars) # gpt2 is around 50000\n",
    "\n",
    "# Encode and decode \n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d35bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([4, 8])\n",
      "Target batch shape: torch.Size([4, 8])\n",
      "Sample input: tensor([26, 27, 30, 32, 20, 33, 25, 14])\n",
      "Sample target: tensor([27, 30, 32, 20, 33, 25, 14, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/3664691306.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/3664691306.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(chunk[1:], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a chunk of text of size block_size + 1\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "    \n",
    "        # Split into input and target\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "tokenized_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = len(tokenized_data)\n",
    "train_data = tokenized_data[:int(n*0.9)]\n",
    "val_data = tokenized_data[int(n*0.9):]\n",
    "\n",
    "train_dataset = CharDataset(train_data, block_size=8)\n",
    "val_dataset = CharDataset(val_data, block_size=8)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "    if batch_idx == 0:  # Just show the first batch\n",
    "        print(\"Input batch shape:\", x_batch.shape)\n",
    "        print(\"Target batch shape:\", y_batch.shape)\n",
    "        print(\"Sample input:\", x_batch[0])\n",
    "        print(\"Sample target:\", y_batch[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf77e9df",
   "metadata": {},
   "source": [
    "## BIgram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(4.8419, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHj\n"
     ]
    }
   ],
   "source": [
    "from torch.fx.node import Target\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramlanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # used to pluck out the embedding of each input idx\n",
    "        # The second dimension is also vocab_size because each token's embedding is used directly as logits\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) (batch, time, channel) (4, 8, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets) # cross entripy exepects (B,C, T)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # Focus on the last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "blm= BigramlanguageModel(vocab_size)\n",
    "logits, loss = blm(x_batch, y_batch)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx = blm.generate(idx, max_new_tokens=50)[0].tolist()\n",
    "print(decode(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b270d22",
   "metadata": {},
   "source": [
    "This is a simplified approach where the model learns to predict the next token based solely on the current token's embedding, without any additional processing. In more complex models (like full Transformers), these embeddings would be further processed through multiple layers of self-attention and feed-forward networks before producing the final logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7283a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/3664691306.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
      "/var/folders/dl/5hxmzpd90yb_8wvp6y4wf2ww0000gn/T/ipykernel_79247/3664691306.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(chunk[1:], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 2.4528, Val Loss: 2.4895\n",
      "Epoch 2/5, Train Loss: 2.4529, Val Loss: 2.4892\n",
      "Epoch 3/5, Train Loss: 2.4529, Val Loss: 2.4897\n",
      "Epoch 4/5, Train Loss: 2.4529, Val Loss: 2.4895\n",
      "Epoch 5/5, Train Loss: 2.4529, Val Loss: 2.4894\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = torch.optim.AdamW(blm.parameters(), lr=1e-3)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    blm.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch, y_batch\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        _, loss = blm(x_batch, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    blm.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch, y_batch\n",
    "            _, loss = blm(x_batch, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, ' \n",
    "          f'Train Loss: {avg_train_loss:.4f}, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe41c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mad, ag.\n",
      "\n",
      "Wat\n",
      "Byelidsomunder olll whatanownthant anonors emaingachy't, pe Yowad LLUSontharo thasowi\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "idx = blm.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(decode(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657b01d",
   "metadata": {},
   "source": [
    "####### The loss plateaus at 2.45. The model predictions do look like words but it still sucks because we are only predicting based on a context of one single token.\n",
    "the objective is to broaden this context and see how that enhances our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5df8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2494aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f5648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80b61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489ace9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f6f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc34c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e654076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57712def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c009d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66493dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f73832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85b381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c7fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b9b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae72017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f169539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3112b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d346b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0cd44e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713325b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f1e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c438d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a9c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f3848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
