{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cddxmMbDXSY1",
        "outputId": "d6b9288d-28a1-4c1b-a980-33b12000231e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-30 20:57:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-09-30 20:57:23 (36.3 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoSgvrfwYS_Y",
        "outputId": "b0a2ee3a-8564-458c-dc96-4419078b1a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7VS8EfOkYDdY"
      },
      "outputs": [],
      "source": [
        "input_file_path = 'input.txt'\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "# this is just for character-level tokentization as i only have a mac M4\n",
        "# Sentencepiece is whats used commonly within the NLP community\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars) # gpt2 is around 50K dimensional embeddings\n",
        "\n",
        "# Encode  decode\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data: torch.Tensor, block_size: int):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data) - self.block_size - 1\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        return chunk[:-1].clone().to(device), chunk[1:].clone().to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lY0zpyT0YEiY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * num_heads == embed_size\n",
        "        ), \"Embedding size must be divisible by number of heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size = x.shape[0]\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Split the embedding into num_heads different pieces\n",
        "        x = x.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        values = self.values(x)\n",
        "        keys = self.keys(x)\n",
        "        queries = self.queries(x)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attention = torch.einsum(\"bqhd,bkhd->bhqk\", [queries, keys])\n",
        "        attention = attention / (self.embed_size ** (1/2))\n",
        "\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Apply softmax\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "\n",
        "        out = torch.einsum(\"bhql,blhd->bqhd\", [attention, values])\n",
        "        out = out.reshape(batch_size, seq_len, self.embed_size)\n",
        "\n",
        "        return self.fc_out(out)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_size, embed_size)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention = self.attention(x, mask)\n",
        "        x = self.norm1(attention + x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        forward = self.feed_forward(x)\n",
        "        x = self.norm2(forward + x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_embedding = nn.Embedding(block_size, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        batch_size, seq_len = x.shape\n",
        "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        x = self.embedding(x) + self.pos_embedding(positions)\n",
        "\n",
        "        # Create a causal mask\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len).to(x.device)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        logits = self.fc_out(x)\n",
        "\n",
        "        if targets is None:\n",
        "            return logits, None\n",
        "\n",
        "        B, T, C = logits.shape\n",
        "        logits_flat = logits.view(B*T, C)\n",
        "        targets_flat = targets.view(-1)\n",
        "        loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                # Crop idx to the last block_size tokens\n",
        "                idx_cond = idx[:, -self.block_size:]\n",
        "                logits, _ = self(idx_cond)\n",
        "                logits = logits[:, -1, :]  # Get the last time step\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "                idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FwuRhSF2YElY"
      },
      "outputs": [],
      "source": [
        "## hyperparams:\n",
        "block_size= 32\n",
        "batch_size = 32\n",
        "embed_size=64\n",
        "num_heads=4\n",
        "num_layers=4\n",
        "learning_rate=3e-4\n",
        "\n",
        "\n",
        "tokenized_data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
        "n = len(tokenized_data)\n",
        "train_data = tokenized_data[:int(n*0.9)]\n",
        "val_data = tokenized_data[int(n*0.9):]\n",
        "\n",
        "train_dataset = CharDataset(train_data.cpu(), block_size = block_size)  # Keep data on CPU for DataLoader\n",
        "val_dataset = CharDataset(val_data.cpu(), block_size = block_size)\n",
        "\n",
        "# Create data loaders\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Create model and move to device\n",
        "# Initialize model\n",
        "model = SimpleTransformer(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_size=embed_size,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    block_size=block_size\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3jzDIuGXdnBG",
        "outputId": "f8f0702c-954e-4bcd-8695-89477b55c916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training: \n",
            "Epoch 1 Train Loss: 2.1601704824719414 Val Loss: 1.9464015779057398\n",
            "Epoch 2 Train Loss: 1.9116269736598546 Val Loss: 1.844886560522159\n",
            "\n",
            "\n",
            "AAPUTOS:\n",
            "An my I foully that for breamber.\n",
            "What dreign, he the but appet about jurght,\n",
            "But me as of\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Training\n",
        "num_epochs = 2\n",
        "print(\"Starting Training: \")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(x_batch, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for x_batch, y_batch in val_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            _, loss = model(x_batch, y_batch)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}',\n",
        "          f'Train Loss: {train_loss/len(train_loader)}',\n",
        "          f'Val Loss: {val_loss/len(val_loader)}')\n",
        "\n",
        "# Generate text\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=100)\n",
        "print(decode(generated[0].cpu().tolist()))  # Move back to CPU for decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drEtYgbGmoqj",
        "outputId": "d23bfc82-59eb-47d0-d7dd-eb63e22257fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Mederperath that you hour. O milthering; I am appOn doonous his all thou me; matt away they the quatsher fay.\n",
            "\n",
            "Fill:\n",
            "\n",
            "SICINIO:\n",
            "Fir have shall poing till Meciady at him: shop that we can voble on these imp\n",
            "Had me comewe to young, brotherings that tirte\n",
            "to stroubful forth un this Hengmy and the lady.\n",
            "\n",
            "UCKINGHAM:\n",
            "I\n",
            "Tild sring in the have godgam!\n",
            "\n",
            "ROMEO:\n",
            "Give, to--tell. Now, sir, her fath in father at thee breath!\n",
            "What to the joyint; stamor: I you wear,\n",
            "For And beam, me stonsict all have cale boy longs Nay.\n",
            "\n",
            "man RICHARD III:\n",
            "as I tenter heare with did your Rickion.\n",
            "Have resthy may vartue back me is your nobt,\n",
            "For the reeds, my and too gon agood the geen are the host,\n",
            "Not is, my sould do save: nay, that comme sheet we beford\n",
            "will me this be him our all argase:\n",
            "Fake teech or see riete!\n",
            "Ang Pher hour strason, rothought?\n",
            "\n",
            "First Cusizent then this have a be dive entlelt flay.\n",
            "\n",
            "QUEEN MARGARE:\n",
            "Be of mave and yearted.\n",
            "\n",
            "For you.\n",
            "\n",
            "SicINIUS:\n",
            "For thy of thee, my no our dids;\n",
            "I he seeptal cronseds yet.\n",
            "\n",
            "QUEEN MOW:\n",
            "Lost goodath our thereirly, gram to erstise,\n",
            "Everer and we Comewers for brown,\n",
            "Have had the jestrest in sea---Signes mead will but them.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Or our sirrow and the lese!\n",
            "\n",
            "KING EDWARD II:\n",
            "Rich'd roof and wis I do me Of of peachal\n",
            "Than to Edward to Come, for which my not your, let all cepoory, o' the tuich and latt likes,\n",
            "But Eingul dow.\n",
            "\n",
            "BENVOTUS:\n",
            "With all be not very great.\n",
            "\n",
            "GLOUCESTER:\n",
            "Now but have charwn, all you this day;\n",
            "Where think a wresserced, I wilter\n",
            "Come nead and you knowrow ear in the lie\n",
            "Nuldes is is hind agrost. good must herbt pritce of doefur juck mide.\n",
            "And piasises's the do me it thy pomst.\n",
            "O, good men's of the kill.\n",
            "\n",
            "Gives EDWARD IV:\n",
            "\n",
            "BENVOLIO:\n",
            "Pay, name tell thun my wentrom.\n",
            "\n",
            "ESCALUS:\n",
            "Shall his channave yevelvaly your see them my not those?\n",
            "I clown in I wadd by you mline!\n",
            "\n",
            "POLIXENS:\n",
            "Which beef, Iake will throught thee of Claffark you will mesterch,\n",
            "nol to threaving this\n",
            "From thought, soin: that the, by the true cone.\n",
            "\n",
            "LUCIONIO:\n",
            "I grave let's y\n"
          ]
        }
      ],
      "source": [
        "# Generate text\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=2000)\n",
        "print(decode(generated[0].cpu().tolist()))  # Move back to CPU for decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r43EOLltmsG6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
